---
title: "Main"
author: "Chengliang Tang, Yujie Wang, Tian Zheng"
output:
  html_document:
    df_print: paged
---
In your final repo, there should be an R markdown file that organizes **all computational steps** for evaluating your proposed Facial Expression Recognition framework. 

This file is currently a template for running evaluation experiments. You should update it according to your codes but following precisely the same structure. 

```{r message=FALSE}
if(!require("EBImage")){
  source("https://bioconductor.org/biocLite.R")
  biocLite("EBImage")
}
if(!require("R.matlab")){
  install.packages("R.matlab")
}
if(!require("readxl")){
  install.packages("readxl")
}

if(!require("dplyr")){
  install.packages("dplyr")
}
if(!require("readxl")){
  install.packages("readxl")
}

if(!require("ggplot2")){
  install.packages("ggplot2")
}

if(!require("caret")){
  install.packages("caret")
}


if(!require("OpenImageR")){
  install.packages("OpenImageR")
}

if(!require("FSelectorRcpp")){
  install.packages("FSelectorRcpp")
}

if(!require("mlr")){
  install.packages("mlr")
}

if(!require("kernlab")){
  install.packages("kernlab")
}

library(R.matlab)
library(readxl)
library(dplyr)
library(EBImage)
library(ggplot2)
library(caret)
library(OpenImageR)
library(FSelectorRcpp)
library(mlr)
library(kernlab)
```

### Step 0 set work directories, extract paths, summarize
```{r wkdir, eval=FALSE}
set.seed(0)
# setwd("~/Project3-FacialEmotionRecognition/doc")
# here replace it with your own path or manually set it in RStudio to where this rmd file is located. 
# use relative path for reproducibility
```

Provide directories for training images. Training images and Training fiducial points will be in different subfolders. 
```{r}
train_dir <- "../data/train_set/" # This will be modified for different data sets.
train_image_dir <- paste(train_dir, "images/", sep="")
train_pt_dir <- paste(train_dir,  "points/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="") 
```


### Step 1: set up controls for evaluation experiments.

In this chunk, we have a set of controls for the evaluation experiments. 

+ (T/F) cross-validation on the training set
+ (number) K, the number of CV folds
+ (T/F) process features for training set
+ (T/F) run evaluation on an independent test set
+ (T/F) process features for test set

```{r exp_setup}
run.cv=TRUE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train=FALSE # process features for training set
run.test=FALSE # run evaluation on an independent test set
run.feature.test=FALSE # process features for test set
run.feature.test.test=TRUE # process features for test_test set
```
Using cross-validation or independent test set evaluation, we compare the performance of models with different specifications. In this Starter Code, we tune parameter k (number of neighbours) for KNN.

```{r model_setup}
k = c(5,11,21,31,41,51)
model_labels = paste("KNN with K =", k)
```

### Step 2: import data and train-test split 
```{r}
#train-test split
info <- read.csv(train_label_path)
n <- nrow(info)
n_train <- round(n*(4/5), 0)
train_idx <- sample(info$Index, n_train, replace = F)
test_idx <- setdiff(info$Index,train_idx)
```

If you choose to extract features from images, such as using Gabor filter, R memory will exhaust all images are read together. The solution is to repeat reading a smaller batch(e.g 100) and process them. 
```{r}
n_files <- length(list.files(train_image_dir))

image_list <- list()
for(i in 1:100){
   image_list[[i]] <- readImage(paste0(train_image_dir, sprintf("%04d", i), ".jpg"))
}
```

Fiducial points are stored in matlab format. In this step, we read them and store them in a list.
```{r read fiducial points}
#function to read fiducial points
#input: index
#output: matrix of fiducial points corresponding to the index
readMat.matrix <- function(index){
     return(round(readMat(paste0(train_pt_dir, sprintf("%04d", index), ".mat"))[[1]],0))
}

#load fiducial points
fiducial_pt_list <- lapply(1:n_files, readMat.matrix)
save(fiducial_pt_list, file="../output/fiducial_pt_list.RData")
```

### Step 3: feature selection


`feature.R` should be the wrapper for all your feature engineering functions and options. The function `feature( )` should have options that correspond to different scenarios for your project and produces an R object that contains features and responses that are required by all the models you are going to evaluate later. 
  
  + `feature.R`
  + Input: list of images or fiducial point
  + Output: an RData file that contains extracted features and corresponding responses

```{r feature}
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
  ## Distance calculation
  tm_feature_train <- system.time(dat_train <- feature_train(fiducial_pt_list, train_idx))
  dat_train <- cbind(dat_train, as.factor(info$emotion_idx[train_idx]))
  colnames(dat_train)[dim(dat_train)[2]] <- "emotion_idx"
  dat_train <- as.data.frame(dat_train)
  colnames(dat_train)<-make.names(colnames(dat_train),unique=T)

  ## Normalize
  tm_feature_train <- tm_feature_train + 
    system.time(dat_train_stand <- feature_normalization(dat_train[,c(-dim(dat_train)[2])]))
 dat_train_stand <- cbind(dat_train_stand,dat_train$emotion_idx)
 colnames(dat_train_stand)[dim(dat_train_stand)[2]] <- "emotion_idx"
 
  ## Feature selection
  tm_feature_train <- tm_feature_train + 
    system.time(feature_name <- feature_selection(dat_train_stand,"emotion_idx"))
  dat_train_selected <- dat_train[,feature_name]
  
  ## Calculate size from all selected distance
  tm_feature_train <- tm_feature_train + 
    system.time(dat_train_double <- feature_selection_size(dat_train_selected))
  
  ## Add manually selected feature
  tm_feature_train <- tm_feature_train + 
    system.time(dat_train_ratio <- manually_feature(fiducial_pt_list, train_idx))
  
  dat_train_selected_stand <- cbind(dat_train_selected,dat_train_double,dat_train_ratio)
  dat_train_selected_stand <- feature_normalization(dat_train_selected_stand)
  dat_train_selected_stand <- cbind(dat_train_selected_stand,dat_train$emotion_idx)
  colnames(dat_train_selected_stand)[dim(dat_train_selected_stand)[2]] <- "emotion_idx"
  
}

tm_feature_test <- NA
if(run.feature.test){
  ## This is the result from test
  feature_name <- c("point.7.to.point.21","point.10.to.point.13","point.10.to.point.33",
                    "point.11.to.point.49","point.12.to.point.55","point.14.to.point.18",
                    "point.23.to.point.50","point.34.to.point.46","point.50.to.point.62",
                    "point.59.to.point.62")
  
  ## Distance Feature
  tm_feature_test <- system.time(dat_test <- feature_train(fiducial_pt_list, test_idx))
  dat_test <- cbind(dat_test, as.factor(info$emotion_idx[test_idx]))
  colnames(dat_test)[dim(dat_test)[2]] <- "emotion_idx"
  dat_test <- as.data.frame(dat_test)
  colnames(dat_test)<-make.names(colnames(dat_test),unique=T)
  dat_test_selected <- dat_test[,feature_name]
  
  ## Size Feature
  tm_feature_test <- tm_feature_test +
    system.time(dat_test_double <- feature_selection_size(dat_test_selected))
  
  ## Add manually selected feature
  tm_feature_test <- tm_feature_test + 
    system.time(dat_test_ratio <- manually_feature(fiducial_pt_list, test_idx))
  
  dat_test_selected_stand <- cbind(dat_test_selected,dat_test_double,dat_test_ratio)
  dat_test_selected_stand <- feature_normalization(dat_test_selected_stand)
  dat_test_selected_stand <- cbind(dat_test_selected_stand,dat_test$emotion_idx)
  colnames(dat_test_selected_stand)[dim(dat_test_selected_stand)[2]] <- "emotion_idx"
}

tm_feature_test_test <- NA
if(run.feature.test.test){
  test_test_idx = c(1:2500)
  ## This is the result from test
  feature_name <- c("point.7.to.point.21","point.10.to.point.13","point.10.to.point.33",
                    "point.11.to.point.49","point.12.to.point.55","point.14.to.point.18",
                    "point.23.to.point.50","point.34.to.point.46","point.50.to.point.62",
                    "point.59.to.point.62")
  
  ## Distance Feature
  tm_feature_test_test <- system.time(dat_test_test <- feature_train(fiducial_pt_list, test_test_idx))
  colnames(dat_test_test)<-make.names(colnames(dat_test_test),unique=T)
  dat_test_test_selected <- dat_test_test[,feature_name]
  
  ## Size Feature
  tm_feature_test_test <- tm_feature_test_test +
    system.time(dat_test_test_double <- feature_selection_size(dat_test_test_selected))
  
  ## Add manually selected feature
  tm_feature_test_test <- tm_feature_test_test + 
    system.time(dat_test_test_ratio <- manually_feature(fiducial_pt_list, test_test_idx))
  
  dat_test_selected_stand_test <- cbind(dat_test_test_selected,dat_test_test_double,
                                   dat_test_test_ratio)
  dat_test_selected_stand_test <- feature_normalization(dat_test_selected_stand_test)
  
}


save(dat_train_selected_stand, file="../output/feature_train.RData")
save(dat_test_selected_stand, file="../output/feature_test.RData")
save(dat_test_selected_stand_test, file="../output/feature_test_test.RData")

```

### Step 4: Train a classification model with training features and responses
Call the train model and test model from library. 

`train.R` and `test.R` should be wrappers for all your model training steps and your classification/prediction steps. 

+ `train.R`
  + Input: a data frame containing features and labels and a parameter list.
  + Output:a trained model
+ `test.R`
  + Input: the fitted classification model using training data and processed features from testing images 
  + Input: an R object that contains a trained classifier.
  + Output: training model specification

+ In this Starter Code, we use KNN to do classification. 

```{r loadlib}
#source("../lib/train.R") Since knn does not need to train, I comment this line.
source("../lib/test_knn.R")
```

#### Model selection with cross-validation
* Do model selection by choosing among different values of training model parameters.
```{r runcv, eval=F}
source("../lib/cross_validation_knn.R")
if(run.cv){
  err_cv <- matrix(0, nrow = length(k), ncol = 2)
  for(i in 1:length(k)){
    cat("k=", k[i], "\n")
    err_cv[i,] <- cv.function(dat_train, K, k[i])
  save(err_cv, file="../output/err_cv.RData")
  }
}
```

Visualize cross-validation results. 
```{r cv_vis}
if(run.cv){
  load("../output/err_cv.RData")
  err_cv <- as.data.frame(err_cv) 
  colnames(err_cv) <- c("mean_error", "sd_error")
  err_cv$k = as.factor(k)
  err_cv %>% 
    ggplot(aes(x = k, y = mean_error,
               ymin = mean_error - sd_error, ymax = mean_error + sd_error)) + 
    geom_crossbar() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
}

```


* Choose the "best" parameter value
```{r best_model}
if(run.cv){
  model_best <- k[which.min(err_cv[,1])]
}
par_best <- list(k = model_best)
```

* Train the model with the entire training set using the selected model (model parameter) via cross-validation.
```{r final_train}
#tm_train=NA
#tm_train <- system.time(fit_train <- train(dat_train, par_best))
#save(fit_train, file="../output/fit_train.RData")
```

### Step 5: Run test on test images
```{r test}
tm_test=NA
if(run.test){
  load(file="../output/fit_train.RData")
  tm_test <- system.time(pred <- test(model_best, dat_test))
}
```

* evaluation
```{r}
accu <- mean(dat_test$emotion_idx == pred)
cat("The accuracy of model:", model_labels[which.min(err_cv[,1])], "is", accu*100, "%.\n")

library(caret)
confusionMatrix(pred, dat_test$emotion_idx)
```

Note that the accuracy is not high but is better than that of ramdom guess(4.5%). 

### Summarize Running Time
Prediction performance matters, so does the running times for constructing features and for training the model, especially when the computation resource is limited. 
```{r running_time}
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
# cat("Time for training model=", tm_train[1], "s \n")
cat("Time for testing model=", tm_test[1], "s \n")
```

###Reference
- Du, S., Tao, Y., & Martinez, A. M. (2014). Compound facial expressions of emotion. Proceedings of the National Academy of Sciences, 111(15), E1454-E1462.