---
title: "Main"
output:
  pdf_document: default
  word_document: default
  html_notebook: default
---
In your final repo, there should be an R markdown file that organizes **all computational steps** for evaluating your proposed Facial Expression Recognition framework. 

This file is currently a template for running evaluation experiments. You should update it according to your codes but following precisely the same structure. 

```{r}
if(!require("EBImage")){
  source("https://bioconductor.org/biocLite.R")
  biocLite("EBImage")
}
if(!require("R.matlab")){
  install.packages("R.matlab")
}
if(!require("readxl")){
  install.packages("readxl")
}

if(!require("dplyr")){
  install.packages("dplyr")
}
if(!require("readxl")){
  install.packages("readxl")
}

if(!require("ggplot2")){
  install.packages("ggplot2")
}

if(!require("caret")){
  install.packages("caret")
}

library(R.matlab)
library(readxl)
library(dplyr)
library(EBImage)
library(ggplot2)
library(caret)
```

### Step 0 set work directories, extract paths, summarize
```{r wkdir, eval=FALSE}
set.seed(0)
setwd("~/Google Drive (yw3285@columbia.edu)/RA/FER/FER/doc")
# here replace it with your own path or manually set it in RStudio to where this rmd file is located. 
# use relative path for reproducibility
```

Notice that images and their annotations are stored in 6 main folders and 12 subfolders, we need to firstly extract paths to reach them.
```{r annotation_path}
path = '../data'
# Use paste to get 6 paths, such as: "../data/01-04/ImageMarking1"
folderpath.anno <- paste(path, 
                         paste(list.files(pattern = "^.{2}-.{2}$",path = path),
                         paste("ImageMarking", 1:6, sep = ""), sep = "/"),
                         sep = "/")
folderpath.anno

# Within each folder, extract file names, return a list of 6 
filename <- lapply(folderpath.anno, list.files, pattern = "\\.mat$")

# Define a function:
# input: "a", "b"
# return: "b/a"
paste.rev <- function(a,b){
  return(paste(b, a, sep = "/"))
}

# Paste folder paths and filenames
annotation_path <- c()
for (i in 1:length(filename)){
  annotation_path <- c(annotation_path, 
                       unlist(lapply(filename[i], paste.rev, folderpath.anno[i])))
}

annotation_path[1:10]
```

```{r image_path}
#image paths
tmp <- gsub(annotation_path, pattern = 'ImageMarking', replacement = 'Images')
image_path <- gsub(tmp, pattern = 'mat',replacement = 'jpg')
image_path[1:10]
```

Summary
```{r}
#Infomation table
categoryID <- substr(annotation_path, 29,30)
categoryID[categoryID<10] <- substr(categoryID[categoryID<10],2,2)
identity <- substr(annotation_path, 32, 34)
emotion_table <- read_xls("../data/AU_annotation_all_subjects.xls")

info <- data.frame(identity, annotation_path, image_path, categoryID = as.numeric(categoryID)) %>%
  left_join(emotion_table, by = c('categoryID' = 'idx')) %>% na.omit() 
info <- info %>% mutate(Index = 1:nrow(info))

head(info)
```

### Step 1: set up controls for evaluation experiments.

In this chunk, we have a set of controls for the evaluation experiments. 

+ (T/F) cross-validation on the training set
+ (number) K, the number of CV folds
+ (T/F) process features for training set
+ (T/F) run evaluation on an independent test set
+ (T/F) process features for test set

```{r exp_setup}
run.cv=TRUE # run cross-validation on the training set
K <- 5  # number of CV folds
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
```
Using cross-validation or independent test set evaluation, we compare the performance of models with different specifications. In this example, we use SVM with different `gamma` and `cost`. In the following chunk, we list, in a matrix, setups corresponding to models that we will compare. In your project, you might compare very different classifiers. You can assign them numerical IDs and labels specific to your project. 

```{r model_setup}
gamma = 10^(-5:-1)
cost = 10^(-1:2)
model_values <- expand.grid(gamma,cost)
model_labels = paste(paste("SVM with gamma =", model_values[,1]),
                     paste(", cost =", model_values[,2]),
                     sep = "")

```

### Step 2: import data and train-test split 
```{r}
#train-test split
n <- nrow(info)
n_train <- round(n*(4/5), 0)
train_idx <- sample(info$Index, n_train, replace = F)
test_idx <- setdiff(info$Index,train_idx)
```

```{r}
Image_list <- lapply(image_path[1:10], EBImage::readImage)
Image_list <- lapply(Image_list, imageData)
```

```{r}
display(Image(Image_list[[1]], colormode = "Color"))
```

```{r}
readMat.matrix <- function(path){
     return(round(readMat(path)[[1]],0))
}

#load fiducial points
fiducial_pt_list <- lapply(annotation_path[1:10], readMat.matrix)
```

```{r}
#display image and fiducial points
Image_list_copy <- Image_list

display_fid_pt <- function(idx, pt_size = 2, pt_col = 1){
for (i in 1:nrow(fiducial_pt_list[[idx]])){
   print(i)
  Image_list_copy[[idx]][
    (fiducial_pt_list[[idx]][i,1]-pt_size):(fiducial_pt_list[[idx]][i,1]+pt_size),
    (fiducial_pt_list[[idx]][i,2]-pt_size):(fiducial_pt_list[[idx]][i,2]+pt_size),] <- pt_col
}
  display(Image(Image_list_copy[[idx]], colormode = 'Color'))
}

display_fid_pt(1, 2, 0)
```

### Step 3: construct features and responses

`feature.R` should be the wrapper for all your feature engineering functions and options. The function `feature( )` should have options that correspond to different scenarios for your project and produces an R object that contains features and responses that are required by all the models you are going to evaluate later. 
+ `feature.R`
  + Input: list of images or fiducial point
  + Output: an RData file that contains extracted features and corresponding responses

```{r feature}
fiducial_pt_list <- lapply(annotation_path, readMat.matrix)
source("../lib/feature.R")
tm_feature_train <- NA
if(run.feature.train){
  tm_feature_train <- system.time(dat_train <- feature(fiducial_pt_list, train_idx))
}

tm_feature_test <- NA
if(run.feature.train){
  tm_feature_test <- system.time(dat_test <- feature(fiducial_pt_list, test_idx))
}

save(dat_train, file="../output/feature_train.RData")
save(dat_test, file="../output/feature_test.RData")
```

### Step 4: Train a classification model with training features and responses
Call the train model and test model from library. 

`train.R` and `test.R` should be wrappers for all your model training steps and your classification/prediction steps. 
+ `train.R`
  + Input: a data frame containing features and labels and a parameter list.
  + Output:a trained model
+ `test.R`
  + Input: the fitted classification model using training data and processed features from testing images 
  + Input: an R object that contains a trained classifier.
  + Output: training model specification
```{r loadlib}
source("../lib/train.R")
source("../lib/test.R")
```

#### Model selection with cross-validation
* Do model selection by choosing among different values of training model parameters, such as gamma and cost.
```{r runcv, eval=F}
source("../lib/cross_validation.R")
if(run.cv){
  err_cv <- array(dim=c(nrow(model_values), 2))
  for(k in 1:nrow(model_values)){
    cat("k=", k, "\n")
    err_cv[k,] <- cv.function(dat_train, K, model_values[k,1], model_values[k,2])
  save(err_cv, file="../output/err_cv.RData")
  }
}
```

Visualize cross-validation results. 
```{r cv_vis}
if(run.cv){
  load("../output/err_cv.RData")
  colnames(model_values) <- c("gamma", "cost")
  colnames(err_cv) <- c("mean_error", "sd_error")
  data.frame(cbind(model_values, err_cv)) %>% 
    mutate(gamma = as.factor(gamma)) %>%
    mutate(cost = paste("cost =",as.factor(cost))) %>%
    ggplot(aes(x = gamma, y = mean_error,
               ymin = mean_error - sd_error, ymax = mean_error + sd_error)) + 
    geom_crossbar() +
    facet_grid(~cost) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
}
```


* Choose the "best" parameter value
```{r best_model}
model_best=model_values[1,]
if(run.cv){
  model_best <- model_values[which.min(err_cv[,1]),]
}
par_best <- list(gamma = model_best$gamma, cost = model_best$cost)
```

* Train the model with the entire training set using the selected model (model parameter) via cross-validation.
```{r final_train}
tm_train=NA
tm_train <- system.time(fit_train <- train(dat_train, par_best))
save(fit_train, file="../output/fit_train.RData")
```

### Step 5: Run test on test images
```{r test}
tm_test=NA
if(run.test){
  load(file="../output/fit_train.RData")
  tm_test <- system.time(pred <- test(fit_train, dat_test))
}
```

* evaluation
```{r}
accu <- mean(dat_test$categoryID == pred)
cat("The accuracy of model:", model_labels[which.min(err_cv[,1])], "is", accu, ".\n")

library(caret)
confusionMatrix(pred, dat_test$categoryID)
```

### Summarize Running Time
Prediction performance matters, so does the running times for constructing features and for training the model, especially when the computation resource is limited. 
```{r running_time}
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training model=", tm_train[1], "s \n")
cat("Time for testing model=", tm_test[1], "s \n")
```

###Reference
- Du, S., Tao, Y., & Martinez, A. M. (2014). Compound facial expressions of emotion. Proceedings of the National Academy of Sciences, 111(15), E1454-E1462.